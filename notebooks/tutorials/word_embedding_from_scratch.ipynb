{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Sophie Katz\n",
    "#\n",
    "# This file is part of Language Model.\n",
    "#\n",
    "# Language Model is free software: you can redistribute it and/or modify it under\n",
    "# the terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "#\n",
    "# Language Model is distributed in the hope that it will be useful, but WITHOUT\n",
    "# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\n",
    "# PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License along with Language\n",
    "# Model. If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing word embedding from scratch using Pytorch\n",
    "\n",
    "This is a minimal implementation of word embedding in PyTorch.\n",
    "\n",
    "## Resources used\n",
    "\n",
    "Name | URL\n",
    "---- | ---\n",
    "Writing a transformer from scratch | https://www.kaggle.com/code/arunmohan003/transformer-from-scratch-using-pytorch\n",
    "\n",
    "## Approach\n",
    "\n",
    "The first step in general for NLP applications is turning our input into a vector. The first step of this is to create word embeddings. There are a number of word embedding models available, but we will be writing our own which will be trained as part of the transformer's training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as T\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "_ = T.manual_seed(57)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The embedding vector for a given word has a fixed size that is not necessarily the\n",
    "# same as the size of the vocabulary. In general, it will usually be much smaller.\n",
    "WORD_EMBEDDING_SIZE = 512\n",
    "\n",
    "# The number of different words we expect to find in our input.\n",
    "VOCABULARY_SIZE = 10000\n",
    "\n",
    "# The length of our input sentence.\n",
    "SENTENCE_LENGTH = 13"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "\n",
    "We will use Pytorch's built-in embedding layer to create our word embedding module. Our module takes as input a tensor of word indices and returns a tensor of word embeddings.\n",
    "\n",
    "Our tensor of word indices should look something like:\n",
    "\n",
    "```python\n",
    "[1 50 82 ...  4 24 98]\n",
    "```\n",
    "\n",
    "It should be of shape `(sentence_length,)`. Likewise, our tensor of word embeddings should be of shape `(sentence_length, embedding_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple word embedding model that takes a word as input and returns its embedding.\n",
    "\n",
    "    This module expects as input a tensor of word indices within the vocabulary of shape\n",
    "    `(sentence_length,)`. It returns a tensor of word embeddings of shape\n",
    "    `(sentence_length, embedding_size)`.\n",
    "\n",
    "    Args:\n",
    "        vocabulary_size: int\n",
    "            The number of different words we expect to find in our input.\n",
    "        embedding_size: int\n",
    "            The size of the embedding vector for a given word.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocabulary_size: int, embedding_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        # We use Pytorch's built in embedding layer\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embedding_size)\n",
    "\n",
    "    def forward(self, sentence: T.Tensor) -> T.Tensor:\n",
    "        # We expect sentence to be of shape (sentence_length,) and to be a tensor of\n",
    "        # word indices within the vocabulary.\n",
    "\n",
    "        assert sentence.ndim == 1\n",
    "\n",
    "        result = self.embedding(sentence)\n",
    "\n",
    "        # We expect result to be of shape (sentence_length, embedding_size)\n",
    "        assert result.ndim == 2\n",
    "        assert result.size(0) == sentence.size(0)\n",
    "        assert result.size(1) == self.embedding_size\n",
    "\n",
    "        return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying it out\n",
    "\n",
    "Let's generate some random data and just run our embedding module on it. The module isn't trained, so this is basically garbage data, but it illustrates how it would work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word indices: tensor([6021, 2019, 8742, 6067, 1243,  509, 9686, 7760, 8596, 5982, 3962, 9773,\n",
      "        8539])\n",
      "Word embeddings shape: torch.Size([13, 512])\n",
      "\n",
      "Word embeddings:\n",
      "tensor([[-1.1089, -1.6561, -0.3991,  ...,  1.1571, -0.0644,  1.8553],\n",
      "        [-2.2252, -0.2858,  0.0437,  ...,  0.8528, -0.7891,  0.1091],\n",
      "        [ 0.8597,  0.4533, -0.3149,  ...,  1.0133, -0.6705,  0.2638],\n",
      "        ...,\n",
      "        [-0.9042, -1.6502, -0.6856,  ...,  2.2120, -1.4461, -0.7706],\n",
      "        [-1.2849,  0.1946, -1.6062,  ..., -1.4948, -2.0231, -0.5197],\n",
      "        [-0.3801, -0.3783, -0.6943,  ..., -0.5142,  0.5159,  1.6972]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create our word embedding module\n",
    "word_embedding = WordEmbedding(VOCABULARY_SIZE, WORD_EMBEDDING_SIZE)\n",
    "\n",
    "# Generate a random sequence of word indices\n",
    "word_indices = T.randint(VOCABULARY_SIZE, (SENTENCE_LENGTH,))\n",
    "\n",
    "print(f\"Word indices: {word_indices}\")\n",
    "\n",
    "# Pass our word indices through the word embedding module to get embedding matrix\n",
    "word_embeddings = word_embedding(word_indices)\n",
    "\n",
    "print(f\"Word embeddings shape: {word_embeddings.shape}\")\n",
    "\n",
    "assert word_embeddings.shape == (SENTENCE_LENGTH, WORD_EMBEDDING_SIZE)\n",
    "\n",
    "print()\n",
    "print(\"Word embeddings:\")\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
