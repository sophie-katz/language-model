{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2023 Sophie Katz\n",
    "#\n",
    "# This file is part of Language Model.\n",
    "#\n",
    "# Language Model is free software: you can redistribute it and/or modify it under\n",
    "# the terms of the GNU General Public License as published by the Free Software\n",
    "# Foundation, either version 3 of the License, or (at your option) any later version.\n",
    "#\n",
    "# Language Model is distributed in the hope that it will be useful, but WITHOUT\n",
    "# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A\n",
    "# PARTICULAR PURPOSE. See the GNU General Public License for more details.\n",
    "#\n",
    "# You should have received a copy of the GNU General Public License along with Language\n",
    "# Model. If not, see <https://www.gnu.org/licenses/>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch.utils.data\n",
    "import torchdata.datapipes as dp\n",
    "from torchtext.datasets import WikiText2\n",
    "\n",
    "from language_model.data.utils.wiki2_transformer import get_wiki2_transformer_datapipe\n",
    "from language_model.models.transformer_from_scratch.transformer_module import (\n",
    "    TransformerModule,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type        | Params\n",
      "--------------------------------------------\n",
      "0 | transformer | Transformer | 120 M \n",
      "--------------------------------------------\n",
      "120 M     Trainable params\n",
      "0         Non-trainable params\n",
      "120 M     Total params\n",
      "481.601   Total estimated model params size (MB)\n",
      "c:\\Users\\sophi\\Code\\language-model\\.venv\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\sophi\\Code\\language-model\\.venv\\lib\\site-packages\\torchdata\\datapipes\\iter\\util\\header.py:57: UserWarning: The length of this HeaderIterDataPipe is inferred to be equal to its limit.The actual value may be smaller if the actual length of source_datapipe is smaller than the limit.\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9130a66bcd3406197d10e47db23fdd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sophi\\Code\\language-model\\language_model\\models\\transformer_from_scratch\\attention.py:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  weight = F.softmax(score / qkv.feature_count**0.5)\n",
      "c:\\Users\\sophi\\Code\\language-model\\.venv\\lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\combining.py:297: UserWarning: Some child DataPipes are not exhausted when __iter__ is called. We are resetting the buffer and each child DataPipe will read from the start again.\n",
      "  warnings.warn(\"Some child DataPipes are not exhausted when __iter__ is called. We are resetting \"\n",
      "`Trainer.fit` stopped: `max_epochs=5` reached.\n"
     ]
    }
   ],
   "source": [
    "train, val, test = WikiText2()\n",
    "\n",
    "vocabulary, train_datapipe = get_wiki2_transformer_datapipe(train)\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "_, val_datapipe = get_wiki2_transformer_datapipe(val, vocabulary=vocabulary)\n",
    "_, test_datapipe = get_wiki2_transformer_datapipe(test, vocabulary=vocabulary)\n",
    "\n",
    "train_datapipe = dp.iter.Header(train_datapipe, 100)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_datapipe)\n",
    "\n",
    "transformer_module = TransformerModule(len(vocabulary))\n",
    "\n",
    "logger = L.pytorch.loggers.TensorBoardLogger(\"logs\", name=\"transformer_from_scratch\")\n",
    "\n",
    "trainer = L.Trainer(max_epochs=2, logger=logger)\n",
    "\n",
    "trainer.fit(transformer_module, train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
